# bafoGPT

üåê [Homepage](#) | ü§ó [Hub](#) | üìñ [arXiv](#) | [GitHub](https://github.com/Motsepe-Jr/bafoGPT)

## Overview

Introducing the open-source Zulu models called BafoGT-2-2B-base and BafoGT-2-2B-it. These models, are a derivation of  **Gemma-2-2b** architecture, have undergone continual pre-training involving approximately 200 Million tokens over a duration of 36 hours with 4 L40 GPUs. At a cost of less than R10-000, achieving similar results to those that cost millions of dollars to pretrain from scratch. It is licensed under the Gemma-2 license and Apache 2.0 License. A permissive license allowing for commercial use and innovation. 

BafoGPT introduces open-source Zulu models: BafoGT-2-2B-base and BafoGT-2-2B-it. These models, are a derivation of  **Gemma-2-2b** architecture and have been continually pre-trained on 200 million tokens over 36 hours using 4 L40 GPUs. With a cost of less than R10,000, BafoGPT delivers results comparable to models that typically require millions of dollars to pretrain from scratch.

Licensed under the permissive Gemma-2 and Apache 2.0 licenses, these models support both commercial use and further innovation. BafoGT-2-2B-base is designed for both IsiZulu and English languages, promoting research and innovation in African AI development. We hope our work inspires further contributions and advancements in this space.

## News

üî• **[2024/08/20]:** The pretraining code has been released, and we also invite you to follow our Repo. üòÜ

## Main Contributions of this Project

- **Vocab Expansion**: Expanded the Gemma-2-2B vocabulary size with 40,000 Zulu tokens for enhanced encoding/decoding efficiency.
- **Largest IsiZulu Dataset**: Open-sourced the largest IsiZulu supervised fine-tuning and pretraining datasets.
- **Large-Scale Data Collection and Processing Scripts**: Includes a separate repository for collecting, cleansing, and processing datasets.
- **Open Source**: Open sourcing both BafoGT-2-2B-base and BafoGT-2-2B-it, Our work is made possible by the open-source community. Special thanks to the teams behind [LitGPT](https://github.com/Lightning-AI/litgpt) and [Google's research](https://arxiv.org/pdf/2403.08295).

